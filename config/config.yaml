# Main configuration file for Human Pose Estimation Pretraining

# Experiment settings
experiment:
  name: h36m_unsupervised_pretrain
  description: "Unsupervised pretraining for Human3.6M dataset"
  seed: 42
  debug: false

# Paths
paths:
  dataset: "ML dataset"
  output_dir: "outputs"
  model_dir: "${paths.output_dir}/models"
  log_dir: "${paths.output_dir}/logs"
  results_dir: "${paths.output_dir}/results"
  pretrained_model: null  # Path to pretrained weights if continuing training

# Dataset configuration
data:
  dataset: "h36m"  # Options: h36m, mpi_inf_3dhp
  config_file: "config/data/h36m.yaml"
  batch_size: 16
  num_workers: 4
  pin_memory: true
  shuffle: true
  sequence_length: 1  # Set > 1 for sequence data
  stride: 1
  keypoint_type: "both"  # Options: 2d, 3d, both
  preload: false  # Whether to preload data in memory

input_type: "keypoints_2d"
target_type: "keypoints_3d"
# Input/Output dimensions
input_dim: 2  # 2D pose
output_dim: 3  # 3D pose
num_joints: 133

# Model configuration
model:
  name: "pretrain_model"
  config_file: "config/models/pretrain_model1.yaml"

training:
  epochs: 1
  learning_rate: 0.0001
  weight_decay: 0.0001
  grad_clip: 1.0
  mixed_precision: true
  resume: false
  early_stopping:
    patience: 10
    min_delta: 0.0001
  optimizer:
    type: "adam"
    beta1: 0.9
    beta2: 0.999
  lr_scheduler:
    type: "cosine"
    warmup_epochs: 5
  losses:
    reconstruction:
      weight: 1.0
      type: "l1"
    consistency:
      weight: 0.5
      type: "cosine"
    smoothness:
      weight: 0.1
      type: "l1"
  adversarial:
    enabled: false
    discriminator:
      hidden_dims: [512, 256]
      learning_rate: 0.0001
    generator_weight: 0.01
  

# Logging and monitoring
logging:
  log_interval: 10  # Log training stats every N batches
  val_interval: 1  # Run validation every N epochs
  checkpoint_interval: 5  # Save model checkpoint every N epochs
  keep_last_n_checkpoints: 3
  tensorboard: true
  wandb:
    use: false
    project: "hpe_pretrain"
    entity: null

# Distributed training
distributed:
  use: false
  backend: "nccl"
  world_size: 1 