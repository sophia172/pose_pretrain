# Configuration for unsupervised pretraining model

name: "pretrain_model1"
description: "Autoencoder-based unsupervised pretraining model for Human3.6M"

# Architecture
architecture:
  type: "transformer"  # Options: mlp, cnn, transformer
  
  # Encoder (2D to latent)
  encoder:
    type: "transformer"
    embedding_dim: 256
    hidden_dims: [1024, 512]
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    positional_encoding: true
    use_joint_relations: true  # Use structural information about joints
    activation: "gelu"  # Options: relu, gelu, silu
  
  # Decoder (latent to 3D)
  decoder:
    type: "transformer"
    hidden_dims: [1024, 1024]
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    activation: "gelu"
  
  # Latent space
  latent:
    dim: 256
    regularization: "none"  # Options: none, l1, l2, kl
    
  # Sequence modeling (if using temporal information)
  sequence:
    enabled: false
    type: "lstm"  # Options: lstm, gru, transformer
    hidden_dim: 512
    num_layers: 2
    bidirectional: true

# Initialization
initialization:
  type: "xavier"  # Options: normal, xavier, kaiming
  gain: 1.0

# Only model-specific regularization here
regularization:
  dropout: 0.1