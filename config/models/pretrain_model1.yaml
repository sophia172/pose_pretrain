# Configuration for unsupervised pretraining model

name: "pretrain_model1"
description: "Autoencoder-based unsupervised pretraining model for Human3.6M"

# Architecture
architecture:
  type: "transformer"  # Options: mlp, cnn, transformer
  
  # Input/Output dimensions
  input_dim: 2  # 2D pose
  output_dim: 3  # 3D pose
  num_joints: 133
  
  # Encoder (2D to latent)
  encoder:
    type: "transformer"
    embedding_dim: 256
    hidden_dims: [1024, 512]
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    positional_encoding: true
    use_joint_relations: true  # Use structural information about joints
    activation: "gelu"  # Options: relu, gelu, silu
  
  # Decoder (latent to 3D)
  decoder:
    type: "transformer"
    hidden_dims: [512, 1024]
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    activation: "gelu"
  
  # Latent space
  latent:
    dim: 256
    regularization: "none"  # Options: none, l1, l2, kl
    
  # Sequence modeling (if using temporal information)
  sequence:
    enabled: false
    type: "lstm"  # Options: lstm, gru, transformer
    hidden_dim: 512
    num_layers: 2
    bidirectional: true

# Training specifics for this model
training:
  batch_size: 64
  learning_rate: 0.0001
  weight_decay: 0.0001
  
  # Learning rate scheduler
  lr_scheduler:
    type: "cosine"
    warmup_epochs: 5
  
  # Specialized losses for pretraining
  losses:
    reconstruction:
      weight: 1.0
      type: "l1"
    consistency:
      weight: 0.5
      type: "cosine"  # For consistency between different augmentations
    smoothness:
      weight: 0.1
      type: "l1"  # Temporal smoothness loss
  
  # Adversarial training (if used)
  adversarial:
    enabled: false
    discriminator:
      hidden_dims: [512, 256]
      learning_rate: 0.0001
    generator_weight: 0.01

# Initialization
initialization:
  type: "xavier"  # Options: normal, xavier, kaiming
  gain: 1.0

# Optimization settings
optimizer:
  type: "adam"  # Options: sgd, adam, adamw
  beta1: 0.9
  beta2: 0.999
  
# Regularization
regularization:
  dropout: 0.1
  weight_decay: 0.0001
  gradient_clip: 1.0 